{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import json\n",
    "import unicodedata\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and store the word2vec model in binary format for fasting loading in the future\n",
    "# model = KeyedVectors.load_word2vec_format('../wiki-news-300d-1M.vec')\n",
    "# model.save_word2vec_format('../wiki-news-300d-1M.vec.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('../wiki-news-300d-1M.vec.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Schadenfreude', 0.7849401831626892)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('schadenfreude', topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and val data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # needed to consider splitting words when there's only one list in word\n",
    "data_dir = 'H:\\\\Users\\\\noyana\\\\Documents\\\\Projects\\\\wordnik'\n",
    "valid_listed_word_path = os.path.join(*[data_dir, 'new_valid_list_data', 'new_valid_list_data', 'valid_listed_words'])\n",
    "valid_listed_words = []\n",
    "with open(valid_listed_word_path, 'r', encoding=\"utf8\") as f:\n",
    "    valid_listed_words = json.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_listed_words = defaultdict(list)\n",
    "for w in valid_listed_words:\n",
    "    wordListId = w['wordListId']['$numberLong']\n",
    "    creatorId = w['creatorId']['$numberLong']\n",
    "    #word = stemmer.stem(w['word'])\n",
    "    \n",
    "    # replace all unicode space \\xa0 with space\n",
    "    word = unicodedata.normalize('NFKD', w['word']) \n",
    "    if len(re.sub('[0-9]|~|!|@|#|\\$|%|\\^|&|\\*|\\(|\\)|-|_|\\+|=|[|{|]|];|:|\\\"|\\'|,|<|>|\\.|\\/|\\?|\\\\\\\\|\\|', '', word)) != len(word):\n",
    "        continue\n",
    "    all_listed_words[int(wordListId)].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30620"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_listed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out word lists that contain only one word\n",
    "word_lists = [wl for wordlistId, wl in all_listed_words.items() if len(wl) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phatic',\n",
       " 'macerate',\n",
       " 'amanuenses',\n",
       " 'theophagy',\n",
       " 'seraglio',\n",
       " 'geophagy',\n",
       " 'metaphone',\n",
       " 'anastrophe',\n",
       " 'neologism',\n",
       " 'tetragrammaton',\n",
       " 'bête noire',\n",
       " 'ablutophobia',\n",
       " 'picayune',\n",
       " 'colophon',\n",
       " 'huzzah',\n",
       " 'embiggen',\n",
       " 'steganography',\n",
       " 'breezer',\n",
       " 'consigliere',\n",
       " 'polari',\n",
       " 'mook',\n",
       " 'synechdoche',\n",
       " 'shrubbery',\n",
       " 'interrobang',\n",
       " 'nychthemeron',\n",
       " 'lagniappe',\n",
       " 'piss and vinegar',\n",
       " 'poetaster',\n",
       " 'shoegazer',\n",
       " 'errata',\n",
       " 'bollocks',\n",
       " 'bookmarklet',\n",
       " 'titification',\n",
       " 'psychopomp',\n",
       " 'gloaming',\n",
       " 'dirigible',\n",
       " 'twee',\n",
       " 'epeolatry',\n",
       " 'strappleberry',\n",
       " 'hemidemisemiquaver',\n",
       " 'prepicenter',\n",
       " 'faineant',\n",
       " 'misandrist',\n",
       " 'festivus',\n",
       " 'cephalophore',\n",
       " 'noosphere',\n",
       " 'taw',\n",
       " 'fulking',\n",
       " 'ghoti',\n",
       " 'bracket',\n",
       " 'smurph',\n",
       " 'refenestration',\n",
       " 'westing',\n",
       " 'provocateur',\n",
       " 'pâté',\n",
       " 'pâte',\n",
       " 'pate',\n",
       " 'john',\n",
       " 'clown',\n",
       " 'poutine',\n",
       " 'hierophant',\n",
       " 'bivy',\n",
       " 'nonce',\n",
       " 'natches',\n",
       " 'bummalo',\n",
       " 'entomologist',\n",
       " 'etymologist',\n",
       " 'headword',\n",
       " 'rfe',\n",
       " 'mitzvah',\n",
       " 'anhedonia',\n",
       " 'piker',\n",
       " 'surcease',\n",
       " 'hedcut',\n",
       " 'transom',\n",
       " 'gyrovague',\n",
       " 'frickin',\n",
       " 'sibilance',\n",
       " 'ineluctable',\n",
       " 'avant',\n",
       " 'crenation',\n",
       " 'jipijapa',\n",
       " 'cuntstruck',\n",
       " 'speciality',\n",
       " 'gaytard',\n",
       " 'plushie',\n",
       " 'sodality',\n",
       " 'litotes',\n",
       " 'findability',\n",
       " 'shirty',\n",
       " 'painbow',\n",
       " 'kabosh',\n",
       " 'coconut',\n",
       " 'blower',\n",
       " 'ballerino',\n",
       " 'frotage',\n",
       " 'breeder',\n",
       " 'borstal',\n",
       " 'jewtastic',\n",
       " 'zima',\n",
       " 'myelination',\n",
       " 'perseveration',\n",
       " 'dudemanteau',\n",
       " 'dudemar',\n",
       " 'godspeed',\n",
       " 'systole',\n",
       " 'butty',\n",
       " 'knackered',\n",
       " 'gnomon',\n",
       " 'debuggery',\n",
       " 'lagniappe',\n",
       " 'onomastic',\n",
       " 'sord',\n",
       " 'dopping',\n",
       " 'mu',\n",
       " 'supervenience',\n",
       " 'pennsylvadar',\n",
       " 'squircle',\n",
       " 'hysteresis',\n",
       " 'purell',\n",
       " 'octosquid',\n",
       " 'conworried',\n",
       " 'oughta',\n",
       " 'idempotent',\n",
       " 'orthorhombic',\n",
       " 'anhydrite',\n",
       " 'windsplit',\n",
       " 'beltline',\n",
       " 'backlight',\n",
       " 'dead cat hole',\n",
       " 'hofmeister kink',\n",
       " 'norfuk',\n",
       " 'can of corn',\n",
       " 'procuress',\n",
       " 'the clapper',\n",
       " 'encephalitic',\n",
       " 'goodmouth',\n",
       " 'badmouth',\n",
       " 'lerve',\n",
       " 'twitter',\n",
       " 'creampt',\n",
       " 'foobris',\n",
       " 'bananaphone',\n",
       " 'placeholder name',\n",
       " 'above the fold',\n",
       " 'hydrocrystalophone',\n",
       " 'crystal baschet',\n",
       " 'hail mary',\n",
       " 'la nuit américaine',\n",
       " 'pasteup',\n",
       " 'photostat',\n",
       " 'cold type',\n",
       " 'halftone',\n",
       " 'golden calf',\n",
       " 'chav',\n",
       " 'backassward',\n",
       " 'pie',\n",
       " 'esprit de corp',\n",
       " 'brass tacks',\n",
       " 'curry western',\n",
       " 'polyseme',\n",
       " 'doucheoisie',\n",
       " 'brannock device',\n",
       " 'love',\n",
       " 'bridge and tunnel',\n",
       " 'spamish',\n",
       " 'lowrider',\n",
       " 'pididdle',\n",
       " 'compa',\n",
       " 'mesonet',\n",
       " 'mesoscale',\n",
       " 'kallawaya',\n",
       " 'skels',\n",
       " 'gomer',\n",
       " 'galactagogue',\n",
       " 'splenomegaly',\n",
       " 'hyperfunction',\n",
       " 'rockism',\n",
       " 'jus ad bellum',\n",
       " 'jus in bello',\n",
       " 'donkeyman',\n",
       " 'tump',\n",
       " 'endian',\n",
       " 'imstupible',\n",
       " 'innit',\n",
       " 'optempo',\n",
       " 'ninja loan',\n",
       " 'stoozing',\n",
       " 'vaccicide',\n",
       " 'manlaughter',\n",
       " 'fake steve jobs',\n",
       " 'gothloli',\n",
       " 'bécasse',\n",
       " 'afterpain',\n",
       " 'meconium',\n",
       " 'eutopia',\n",
       " 'gimson',\n",
       " 'notary sojac',\n",
       " 'dl',\n",
       " 'tostications',\n",
       " 'touchdown',\n",
       " 'txtspk',\n",
       " 'angstrom',\n",
       " 'callipygian',\n",
       " 'herzog',\n",
       " 'clippy',\n",
       " 'adhan',\n",
       " 'fard',\n",
       " 'salah',\n",
       " 'farida',\n",
       " 'ahkam',\n",
       " 'yill',\n",
       " 'camra',\n",
       " 'ibu',\n",
       " 'krusening',\n",
       " 'kräusening',\n",
       " 'lauter',\n",
       " 'malt liquor',\n",
       " 'shmita',\n",
       " 'heter mechira',\n",
       " 'gilding the lily',\n",
       " 'beer thirty',\n",
       " 'glengarry',\n",
       " 'toorie',\n",
       " 'thunder chicken',\n",
       " 'cluetrain',\n",
       " 'textual harassment',\n",
       " 'coaming',\n",
       " 'word apnea',\n",
       " 'ey',\n",
       " 'spivak',\n",
       " 'eir',\n",
       " 'xe',\n",
       " 've',\n",
       " 'zie',\n",
       " 'thon',\n",
       " 'herself',\n",
       " 'themselves',\n",
       " 'xyr',\n",
       " 'ossicusp',\n",
       " 'towelie',\n",
       " 'minced oath',\n",
       " 'ymmv',\n",
       " 'corpora',\n",
       " 'cheesewa',\n",
       " 'cheesois',\n",
       " 'gooble',\n",
       " 'glossaralia',\n",
       " 'sugar snow',\n",
       " 'sugar bush',\n",
       " 'straight last',\n",
       " 'sugar nipper',\n",
       " 'franklin stove',\n",
       " 'bone char',\n",
       " 'sockdologize',\n",
       " 'mclovin',\n",
       " 'overwatch',\n",
       " 'bugs',\n",
       " 'autocomplete',\n",
       " 'slew rate',\n",
       " 'jumpman',\n",
       " 'meth',\n",
       " 'merch',\n",
       " 'chi rho',\n",
       " 'depth charge',\n",
       " 'ashcan',\n",
       " 'brazillion',\n",
       " 'lysistrada',\n",
       " 'in flagrante',\n",
       " 'annuit coeptis',\n",
       " 'eye of providence',\n",
       " 'all seeing eye',\n",
       " 'lysistrata',\n",
       " 'banger',\n",
       " 'nagapie',\n",
       " 'bennifer',\n",
       " 'subsession',\n",
       " 'splittist',\n",
       " 'strabo',\n",
       " 'multislacking',\n",
       " 'papiamento',\n",
       " 'palenque',\n",
       " 'load average',\n",
       " 'in his cups',\n",
       " 'vancouver',\n",
       " 'spreader',\n",
       " 'bowspirit',\n",
       " 'tragedy of the commons',\n",
       " 'name dropping',\n",
       " 'rum runner',\n",
       " 'scoop jackson democrat',\n",
       " 'sadhu',\n",
       " 'artha',\n",
       " 'glaucoma',\n",
       " 'rank amateur',\n",
       " 'mos def',\n",
       " 'star turn',\n",
       " 'mau mau',\n",
       " 'lexiphanes',\n",
       " 'too pooped to pop',\n",
       " 'yellow card',\n",
       " 'ebitda',\n",
       " 'misery index',\n",
       " 'curvebreaker',\n",
       " 'cavage',\n",
       " 'epsonality',\n",
       " 'drunkalogue',\n",
       " 'completism',\n",
       " 'completist',\n",
       " 'ecomagination',\n",
       " 'greenwash',\n",
       " 'canjet',\n",
       " 'q score',\n",
       " 'hightail',\n",
       " 'serial comma',\n",
       " 'harvard comma',\n",
       " 'question mark',\n",
       " 'quotation marks',\n",
       " 'neutral corners',\n",
       " 'troponym',\n",
       " 'negative corpus',\n",
       " 'wordnet',\n",
       " 'untermensch',\n",
       " 'read into',\n",
       " 'xerophagy',\n",
       " 'anne',\n",
       " 'potato love',\n",
       " 'ahimsa',\n",
       " 'glorious twelfth',\n",
       " 'ovine',\n",
       " 'krokodiloes',\n",
       " 'raif',\n",
       " 'exquisite corpse',\n",
       " 'cyclomatic complexity',\n",
       " 'juliet balcony',\n",
       " 'interrobang',\n",
       " 'geigh',\n",
       " 'mobtown',\n",
       " 'proxivore',\n",
       " 'winchellism',\n",
       " 'suicide by cop',\n",
       " 'fug',\n",
       " 'nodi',\n",
       " 'astroturfing',\n",
       " 'turdblossom',\n",
       " 'mobile',\n",
       " 'sleepsperiences',\n",
       " 'etarded',\n",
       " 'plunderphonics',\n",
       " 'wordie blogs',\n",
       " 'flowbee',\n",
       " 'shit disturber',\n",
       " 'coug',\n",
       " 'frostitute',\n",
       " 'ambiguity device',\n",
       " 'chugly',\n",
       " 'lolcode',\n",
       " 'quinzee',\n",
       " 'fuelie',\n",
       " 'heelys',\n",
       " 'oggsford',\n",
       " 'affinage',\n",
       " 'fillum',\n",
       " 'kill fee',\n",
       " 'jocko homo',\n",
       " 'sausage club',\n",
       " 'hunky punk',\n",
       " 'airing of grievances',\n",
       " 'starchitect',\n",
       " 'déjà fatigué',\n",
       " 'pseudo event',\n",
       " 'earmarxist',\n",
       " 'crowdsource',\n",
       " 'baghdadi',\n",
       " 'moraler',\n",
       " 'curse of knowledge',\n",
       " 'redhots',\n",
       " 'buttboy',\n",
       " 'flexidoxy',\n",
       " 'srsly',\n",
       " 'hymeneal',\n",
       " 'twiver',\n",
       " 'jk',\n",
       " 'krumholtz',\n",
       " 'citjo',\n",
       " 'chuppah',\n",
       " 'deal spread',\n",
       " 'piquenique',\n",
       " 'in posse',\n",
       " 'quotative like',\n",
       " 'global weirding',\n",
       " 'hot link',\n",
       " 'take a flier',\n",
       " 'cranky pants',\n",
       " 'surcingle',\n",
       " 'nanoscopic',\n",
       " 'padawan',\n",
       " 'golgi apparatus',\n",
       " 'flenser',\n",
       " 'chicken chested',\n",
       " 'potato barn',\n",
       " 'pianomatopoeia',\n",
       " 'textist',\n",
       " 'idiopathy',\n",
       " 'repugn',\n",
       " 'migraineur',\n",
       " 'figura',\n",
       " 'figurae',\n",
       " 'causa pro metrica',\n",
       " 'brennschluss',\n",
       " 'marlinespike',\n",
       " 'seabag',\n",
       " 'shore patrol',\n",
       " 'zone of alienation',\n",
       " 'caveat venditor',\n",
       " 'via crucis',\n",
       " 'via dolorosa',\n",
       " 'stations of the cross',\n",
       " 'way of the cross',\n",
       " 'a tergo',\n",
       " 'monkey patch',\n",
       " 'auto de fe',\n",
       " 'shooting skip',\n",
       " 'webshite',\n",
       " 'wyomingite',\n",
       " 'turmoil of squid',\n",
       " 'structuring',\n",
       " 'make time',\n",
       " 'saleschild',\n",
       " 'flockmaster',\n",
       " 'boy george',\n",
       " 'fornever',\n",
       " 'test',\n",
       " 'lessness',\n",
       " 'on about',\n",
       " 'indicium',\n",
       " 'gesture politics',\n",
       " 'flibbity gibbet',\n",
       " 'doppelgoogle',\n",
       " 'elevatoring',\n",
       " 'blind hoistway',\n",
       " 'antibalas',\n",
       " 'young frankenstein',\n",
       " 'margar',\n",
       " 'imaginationalism',\n",
       " 'the cake of custom',\n",
       " 'character entity',\n",
       " 'ghost write',\n",
       " 'ghosting',\n",
       " 'red hot peppers',\n",
       " 'adad',\n",
       " 'erotimime',\n",
       " 'bip',\n",
       " 'moro reflex',\n",
       " 'leccy bill',\n",
       " 'kokomosexual',\n",
       " 'cornucopia of the commons',\n",
       " 'psychic jukebox',\n",
       " 'double double',\n",
       " 'mutato',\n",
       " 'fudgsicle',\n",
       " 'canada day',\n",
       " 'lowerarchy',\n",
       " 'decoupling',\n",
       " 'paracosm',\n",
       " 'volapuck',\n",
       " 'volapuk',\n",
       " 'translit',\n",
       " 'captology',\n",
       " 'attract mode',\n",
       " 'paradoxical sleep',\n",
       " 'nugacity',\n",
       " 'sandwichero',\n",
       " 'silent key',\n",
       " 'overfitting',\n",
       " 'frown of approval',\n",
       " 'spit shine a turd',\n",
       " 'tarp your load',\n",
       " 'microsharing',\n",
       " 'seo',\n",
       " 'dawn patrol',\n",
       " 'wing fence',\n",
       " 'jesus frog',\n",
       " 'nicely disrespectful',\n",
       " 'threadsafe',\n",
       " 'wordia',\n",
       " 'cooking with gas',\n",
       " 'yo la tengo',\n",
       " 'casio core',\n",
       " 'shis',\n",
       " 'president obama',\n",
       " 'big board',\n",
       " 'eupsychian',\n",
       " 'serp',\n",
       " 'sangre de drago',\n",
       " 'no courant',\n",
       " 'hash house harriers',\n",
       " 'hurricane party',\n",
       " 'hoptoad',\n",
       " 'car czar',\n",
       " 'agnotology',\n",
       " 'roller derby',\n",
       " 'cursebird',\n",
       " 'interoperable',\n",
       " 'brinking',\n",
       " 'bananapocalypse',\n",
       " 'emoji',\n",
       " 'pants status',\n",
       " 'privacy policy',\n",
       " 'phalacrocoracidae',\n",
       " 'barococo',\n",
       " 'fantasound',\n",
       " 'friends with benefits',\n",
       " 'fuseful',\n",
       " 'walking bus',\n",
       " 'newordizen',\n",
       " 'albatross that laid the golden egg',\n",
       " 'quartodeciman',\n",
       " 'mobsourcing',\n",
       " 'cattlesnake',\n",
       " 'tico',\n",
       " 'nehru',\n",
       " 'varna',\n",
       " 'jati',\n",
       " 'kshatriya',\n",
       " 'dalit',\n",
       " 'vaishya',\n",
       " 'shudra',\n",
       " 'ayuntamiento',\n",
       " 'motherboy',\n",
       " 'manpoo',\n",
       " 'honq',\n",
       " 'middie',\n",
       " 'kalishnarumpet',\n",
       " 'stylophone',\n",
       " 'career disoriented',\n",
       " 'badarak',\n",
       " 'cantopop',\n",
       " 'pole position',\n",
       " 'quidity',\n",
       " 'modcon',\n",
       " 'aestheatrical',\n",
       " 'tablescaping',\n",
       " 'suchnot',\n",
       " 'limpoma',\n",
       " 'camboy',\n",
       " 'smtoe',\n",
       " 'rfl',\n",
       " 'twitterversosphere',\n",
       " 'earlash',\n",
       " 'afferent',\n",
       " 'starn',\n",
       " 'cham',\n",
       " 'donkey',\n",
       " 'monkey',\n",
       " 'rat',\n",
       " 'theodicy',\n",
       " 'obamulate',\n",
       " 'obambulate',\n",
       " 'railstard']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del all_listed_words\n",
    "del valid_listed_words\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split into train and test ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28208\n",
      "28208\n"
     ]
    }
   ],
   "source": [
    "#word_lists = [wl for wl in word_lists.items() if len(wl) > 4]\n",
    "tmp_list = [\n",
    "    ['michael', 'romeo', 'juliet', 'flowers', 'poet', 'shakespear'],\n",
    "    ['goodday', 'french', 'spanish', 'table', 'golden'],\n",
    "    ['harvard', 'machine', 'learning']\n",
    "]\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for wl in word_lists:\n",
    "    if len(wl) < 4:\n",
    "        continue\n",
    "    else:\n",
    "        test_list.append(wl.pop())\n",
    "        train_list.append(wl)\n",
    "\n",
    "print(len(test_list))\n",
    "print(len(train_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('train_list.json', 'w') as f:\n",
    "    json.dump(train_list, f)\n",
    "with open('test_list.json', 'w') as f:\n",
    "    json.dump(test_list, f)\n",
    "    \n",
    "del word_lists\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write lists to json\n",
    "# with open('word_lists.json', 'w') as f:\n",
    "#     json.dump(word_lists, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create evaluation metric ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "['stud', 'play', 'number']\n",
      "['play'] 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "def eval_rec_word(rec_word_list, test_word):\n",
    "    \"\"\" assumes recommendation function recommends several words\n",
    "    see if any of the recommended word (after stemming) matches the test word\"\"\"\n",
    "    stem_test_word = stemmer.stem(test_word)\n",
    "    stem_rec_word = list(map(lambda w: stemmer.stem(w), rec_word_list))\n",
    "    \n",
    "    # find number of words in recommended word matching test word\n",
    "    match_test_word = list(filter(lambda w: w == stem_test_word, stem_rec_word))\n",
    "    score = len(match_test_word)/len(stem_rec_word)\n",
    "\n",
    "    return score\n",
    "\n",
    "eval_rec_word(['studded', 'played', 'numbers'], 'playing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create word count and tag count in word lists ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cnt_map = {}\n",
    "for wl in word_lists:\n",
    "    for word in wl:\n",
    "        word_cnt_map[word] = word_cnt_map.get(stemmer.stem(word), 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect top 20 words listed in word lists\n",
    "sorted_words = sorted(word_cnt_map.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_words[0:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tags\n",
    "with open('../clean_data/allTaggedItems-06Jan2019.json', 'rb') as f:\n",
    "    tagitem = json.load(f)\n",
    "    \n",
    "word_numtags_map = {}\n",
    "for i in tagitem:\n",
    "    # get default word form\n",
    "    word = stemmer.stem(i['object_id'])\n",
    "    # replace all unicode space \\xa0 with space\n",
    "    word = unicodedata.normalize('NFKD', word) \n",
    "    # remove words that are not composed by alphabets (spaces are ok)\n",
    "    if len(re.sub('[0-9]|~|!|@|#|\\$|%|\\^|&|\\*|\\(|\\)|-|_|\\+|=|[|{|]|];|:|\\\"|\\'|,|<|>|\\.|\\/|\\?|\\\\\\\\|\\|', '', word)) != len(word):\n",
    "        continue\n",
    "    word_numtags_map[word] = word_numtags_map.get(word, 0) + 1\n",
    "    \n",
    "with open('../clean_data/word_numtags_map.json', 'w') as f:\n",
    "    json.dump(word_numtags_map, f)\n",
    "del tagitem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect top 20 words being tagged\n",
    "sorted_wordtags = sorted(word_numtags_map.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_wordtags[0:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sorted_words\n",
    "del sorted_wordtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec's Limitation: Word2Vec may return the exact same word as the query word ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('../wiki-news-300d-1M.vec.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('schadenfreude', topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec may return the exact same word\n",
    "model.most_similar('schadenfreud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('fracked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word recommender that filteres similar words and rank remaining words ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar words in a word list\n",
    "def get_wl_word2vec(model, wl, debug=True):\n",
    "    \"\"\"Take average word2vec for each word in word list\"\"\"\n",
    "    wl_w2vec = []\n",
    "    # for each word get word2vec representation\n",
    "    \n",
    "    for w in wl:\n",
    "        try:\n",
    "            wvec = model[w]\n",
    "            wl_w2vec.append(wvec)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if len(wl_w2vec) == 0:\n",
    "        return np.zeros((300))\n",
    "    else:\n",
    "        return np.mean(np.asarray(wl_w2vec), axis=0)\n",
    "\n",
    "\n",
    "def get_sim_words(model, wl, topn, debug=True):\n",
    "    # retrieve the word2vec representation of the word list\n",
    "    wl_w2vec = get_wl_word2vec(model, wl, debug=debug)\n",
    "    \n",
    "    # return top n similar words\n",
    "    return model.similar_by_vector(wl_w2vec, topn = topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank all the return words\n",
    "def rank_words(stemmer, wl, word_numtags_map, word_cnt_map, word_cnt_weight=0.8, word_tag_weight=0.2, debug=True):\n",
    "    assert(len(wl) > 0)\n",
    "    def score_word(w):\n",
    "        stem_w = stemmer.stem(w)\n",
    "        word_cnt = word_cnt_map.get(stem_w, 0)\n",
    "        word_numtag = word_numtags_map.get(stem_w, 0)\n",
    "        score = word_cnt * word_cnt_weight + word_numtag * word_tag_weight\n",
    "        return score\n",
    "        \n",
    "    wl_scores = list(map(lambda w: score_word(w), wl))\n",
    "    maxidx = np.argmax(wl_scores)\n",
    "    if debug:\n",
    "        print(wl_scores, maxidx)\n",
    "    return wl[np.argmax(wl_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unique(stemmer, wl, sim_wl):\n",
    "    \"\"\" Remove similar words that share the same words in word list\"\"\"\n",
    "    stem_wl = set(map(lambda w: stemmer.stem(w), wl))\n",
    "    return list(filter(lambda w: stemmer.stem(w) not in stem_wl, sim_wl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_words(model, stemmer, wl, word_numtags_map, word_cnt_map, topn=10, word_cnt_weight=0.8, word_tag_weight=0.2, debug=True):    \n",
    "    \"\"\"Recommand a word to add based on word list\"\"\"\n",
    "    assert len(wl) > 0, \"Length of word list needs to be greater than 1\"\n",
    "    \n",
    "    # find similar words to word list\n",
    "    sim_wl = get_sim_words(model, wl, topn=10, debug=debug)\n",
    "    np_sim_wl = np.array(sim_wl)[:,0]\n",
    "    if debug:\n",
    "        print('similar word list', np_sim_wl)\n",
    "\n",
    "    # filter duplicating words\n",
    "    filtered_wl = filter_unique(stemmer, wl, np_sim_wl)\n",
    "    if debug:\n",
    "        print('Filtered word list', filtered_wl)\n",
    "\n",
    "    # rank words\n",
    "    if len(filtered_wl) == 0:\n",
    "        top_w = rank_words(stemmer, wl, word_numtags_map, word_cnt_map, word_cnt_weight=0, word_tag_weight=1, debug=debug)\n",
    "    else:\n",
    "        top_w = rank_words(stemmer, filtered_wl, word_numtags_map, word_cnt_map, word_cnt_weight=0, word_tag_weight=1, debug=debug)\n",
    "    return top_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = ['esper', 'espers', 'B.A.B.E.L.', 'Hyōbu' ,'magic-user', 'kekkai',\n",
    " 'shapechanging', 'magic-using', 'paopei', 'meta-human']\n",
    "#wl = ['schadenfreude', 'ephemeral']\n",
    "recommend_words(model, stemmer, wl, word_numtags_map, word_cnt_map, topn=10, word_cnt_weight=0.8, word_tag_weight=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words = list(map(lambda wl: recommend_words(model, stemmer, wl, word_numtags_map, word_cnt_map, debug=False), word_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pred(model, word_cnt_map, word_numtag_map, pred_w, true_w):\n",
    "    \"\"\"\n",
    "    Performs 3 metrics evaluations\n",
    "    1. cosine similarity between predicted word versus true word. range -1~1\n",
    "    2. word count difference percentage. 0~1\n",
    "    3. tag count difference percentage. 0~1\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pred_w2vec = model[pred_w]\n",
    "    except:\n",
    "        pred_w2vec = np.zeros((300))\n",
    "    try:\n",
    "        true_w2vec = model[true_w]\n",
    "    except:\n",
    "        true_w2vec = np.zeros((300))\n",
    "    if not np.any(pred_w2vec) or not np.any(true_w2vec):\n",
    "        cos_sim = 0\n",
    "    else:\n",
    "        cos_sim = np.dot(pred_w2vec, true_w2vec)/(np.linalg.norm(pred_w2vec) * np.linalg.norm(true_w2vec))\n",
    "    \n",
    "    try:\n",
    "        true_w_wcnt = word_cnt_map[true_w]\n",
    "        pred_w_wcnt = word_cnt_map.get(pred_w, 0)\n",
    "        print('true_w_wcnt', true_w_wcnt, 'pred_w_wcnt', pred_w_wcnt)\n",
    "        cnt_sim = abs(true_w_wcnt - pred_w_wcnt)/true_w_wcnt\n",
    "    except:\n",
    "        cnt_sim = 0\n",
    "    \n",
    "    try:\n",
    "        true_w_tagcnt = word_cnt_map[true_w]\n",
    "        pred_w_tagcnt = word_cnt_map.get(pred_w, 0)\n",
    "        print('true_w_tagcnt', true_w_tagcnt, 'pred_w_tagcnt', pred_w_tagcnt)\n",
    "        tagcnt_sim = abs(true_w_tagcnt - pred_w_tagcnt)/true_w_tagcnt\n",
    "    except:\n",
    "        tagcnt_sim = 0\n",
    "\n",
    "    print('Cosine similarity', cos_sim, 'cnt_sim', cnt_sim, 'tagcnt_sim', tagcnt_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = word_lists[100][-1]\n",
    "wl_len = len(word_lists[100])\n",
    "wl = word_lists[100][0:wl_len-1]\n",
    "print('Word List:', wl)\n",
    "print('Missing word', true_w)\n",
    "pred_w = recommend_words(model, stemmer, wl, word_numtags_map, word_cnt_map, topn=10, word_cnt_weight=0.8, word_tag_weight=0.2)\n",
    "print('predicted word', pred_w)\n",
    "# evaluate prediction\n",
    "eval_pred(model, word_cnt_map, word_numtags_map, pred_w, true_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
