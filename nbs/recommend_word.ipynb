{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import json\n",
    "import unicodedata\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and store the word2vec model in binary format for fasting loading in the future\n",
    "# model = KeyedVectors.load_word2vec_format('../wiki-news-300d-1M.vec')\n",
    "# model.save_word2vec_format('../wiki-news-300d-1M.vec.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('../wiki-news-300d-1M.vec.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Schadenfreude', 0.7849401831626892)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('schadenfreude', topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and val data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # needed to consider splitting words when there's only one list in word\n",
    "data_dir = os.getcwd()\n",
    "valid_listed_word_path = os.path.join(*[data_dir, 'new_valid_list_data', 'valid_listed_words'])\n",
    "valid_listed_words = []\n",
    "with open(valid_listed_word_path, 'r', encoding=\"utf8\") as f:\n",
    "    valid_listed_words = json.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_listed_words = defaultdict(list)\n",
    "for w in valid_listed_words:\n",
    "    wordListId = w['wordListId']['$numberLong']\n",
    "    creatorId = w['creatorId']['$numberLong']\n",
    "    word = stemmer.stem(w['word'])\n",
    "    \n",
    "    # replace all unicode space \\xa0 with space\n",
    "    word = unicodedata.normalize('NFKD', word) \n",
    "    if len(re.sub('[0-9]|~|!|@|#|\\$|%|\\^|&|\\*|\\(|\\)|-|_|\\+|=|[|{|]|];|:|\\\"|\\'|,|<|>|\\.|\\/|\\?|\\\\\\\\|\\|', '', word)) != len(word):\n",
    "        continue\n",
    "    all_listed_words[int(wordListId)].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30620"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_listed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out word lists that contain only one word\n",
    "word_lists = [wl for wordlistId, wl in all_listed_words.items() if len(wl) > 1]\n",
    "del all_listed_words\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30541"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write lists to json\n",
    "with open('word_lists.json', 'w') as f:\n",
    "    json.dump(word_lists, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create word count and tag count in word lists ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cnt_map = {}\n",
    "for wl in word_lists:\n",
    "    for word in wl:\n",
    "        word_cnt_map[word] = word_cnt_map.get(stemmer.stem(word), 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('schadenfreud', 559),\n",
       " ('defenestr', 530),\n",
       " ('sanguin', 524),\n",
       " ('quixot', 523),\n",
       " ('lugubri', 509),\n",
       " ('melliflu', 507),\n",
       " ('love', 488),\n",
       " ('obsequi', 487),\n",
       " ('insouci', 479),\n",
       " ('loquaci', 476),\n",
       " ('lacon', 459),\n",
       " ('loveli', 443),\n",
       " ('ennui', 440),\n",
       " ('ether', 436),\n",
       " ('serendip', 433),\n",
       " ('caprici', 428),\n",
       " ('inchoat', 422),\n",
       " ('loving', 409),\n",
       " ('serendipiti', 408)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect top 20 words listed in word lists\n",
    "sorted_words = sorted(word_cnt_map.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_words[0:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tags\n",
    "with open('../clean_data/allTaggedItems-06Jan2019.json', 'rb') as f:\n",
    "    tagitem = json.load(f)\n",
    "    \n",
    "word_numtags_map = {}\n",
    "for i in tagitem:\n",
    "    # get default word form\n",
    "    word = stemmer.stem(i['object_id'])\n",
    "    # replace all unicode space \\xa0 with space\n",
    "    word = unicodedata.normalize('NFKD', word) \n",
    "    # remove words that are not composed by alphabets (spaces are ok)\n",
    "    if len(re.sub('[0-9]|~|!|@|#|\\$|%|\\^|&|\\*|\\(|\\)|-|_|\\+|=|[|{|]|];|:|\\\"|\\'|,|<|>|\\.|\\/|\\?|\\\\\\\\|\\|', '', word)) != len(word):\n",
    "        continue\n",
    "    word_numtags_map[word] = word_numtags_map.get(word, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tagitem\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('admir', 214),\n",
       " ('about', 201),\n",
       " ('adobeair', 197),\n",
       " ('addon', 195),\n",
       " ('nectareousraconteusenectarouscourtesan', 96),\n",
       " ('love', 58),\n",
       " ('overtag', 53),\n",
       " ('red', 49),\n",
       " ('divers', 48),\n",
       " ('object', 45),\n",
       " ('good', 38),\n",
       " ('wordnik', 35),\n",
       " ('beauti', 34),\n",
       " ('fistfuck', 32),\n",
       " ('isi', 32),\n",
       " ('reflect', 30),\n",
       " ('project', 29),\n",
       " ('express', 29),\n",
       " ('fuck', 28)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect top 20 words being tagged\n",
    "sorted_wordtags = sorted(word_numtags_map.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_wordtags[0:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del sorted_words\n",
    "del sorted_wordtags\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec's Limitation: Word2Vec may return the exact same word as the query word ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Schadenfreude', 0.7849401831626892),\n",
       " ('epicaricacy', 0.6519575715065002),\n",
       " ('gloating', 0.594894528388977),\n",
       " ('smugness', 0.5815554857254028),\n",
       " ('glee', 0.5808501243591309),\n",
       " ('weltschmerz', 0.5751596093177795),\n",
       " ('hate-watching', 0.5631588697433472),\n",
       " ('irony', 0.556583821773529),\n",
       " ('self-mockery', 0.5543162822723389),\n",
       " ('pearl-clutching', 0.5435289144515991)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word2vec may return the exact same word\n",
    "model.most_similar('schadenfreude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('frack', 0.7199307680130005),\n",
       " ('fracks', 0.6991308927536011),\n",
       " ('fracking', 0.6949862241744995),\n",
       " ('frackers', 0.6732550859451294),\n",
       " ('fraccing', 0.659175455570221),\n",
       " ('Fracked', 0.6529004573822021),\n",
       " ('fracing', 0.6463837027549744),\n",
       " ('Fracking', 0.6427013278007507),\n",
       " ('shale-gas', 0.6260367631912231),\n",
       " ('fraking', 0.6246780157089233)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('fracked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word recommender that filteres similar words and rank remaining words ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar words in a word list\n",
    "def get_wl_word2vec(model, wl, debug=True):\n",
    "    \"\"\"Take average word2vec for each word in word list\"\"\"\n",
    "    wl_w2vec = []\n",
    "    # for each word get word2vec representation\n",
    "    \n",
    "    for w in wl:\n",
    "        try:\n",
    "            wvec = model[w]\n",
    "            wl_w2vec.append(wvec)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if len(wl_w2vec) == 0:\n",
    "        return np.zeros((300))\n",
    "    else:\n",
    "        return np.mean(np.asarray(wl_w2vec), axis=0)\n",
    "\n",
    "\n",
    "def get_sim_words(model, wl, topn, debug=True):\n",
    "    # retrieve the word2vec representation of the word list\n",
    "    wl_w2vec = get_wl_word2vec(model, wl, debug=debug)\n",
    "    \n",
    "    # return top n similar words\n",
    "    return model.similar_by_vector(wl_w2vec, topn = topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank all the return words\n",
    "def rank_words(stemmer, wl, word_numtags_map, word_cnt_map, word_cnt_weight=0.8, word_tag_weight=0.2, debug=True):\n",
    "    assert(len(wl) > 0)\n",
    "    def score_word(w):\n",
    "        stem_w = stemmer.stem(w)\n",
    "        word_cnt = word_cnt_map.get(stem_w, 0)\n",
    "        word_numtag = word_numtags_map.get(stem_w, 0)\n",
    "        score = word_cnt * word_cnt_weight + word_numtag * word_tag_weight\n",
    "        return score\n",
    "        \n",
    "    wl_scores = list(map(lambda w: score_word(w), wl))\n",
    "    maxidx = np.argmax(wl_scores)\n",
    "    if debug:\n",
    "        print(wl_scores, maxidx)\n",
    "    return wl[np.argmax(wl_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unique(stemmer, wl, sim_wl):\n",
    "    \"\"\" Remove similar words that share the same words in word list\"\"\"\n",
    "    stem_wl = set(map(lambda w: stemmer.stem(w), wl))\n",
    "    return list(filter(lambda w: stemmer.stem(w) not in stem_wl, sim_wl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_words(model, stemmer, wl, word_numtags_map, word_cnt_map, topn=10, word_cnt_weight=0.8, word_tag_weight=0.2, debug=True):    \n",
    "    \"\"\"Recommand a word to add based on word list\"\"\"\n",
    "    assert len(wl) > 0, \"Length of word list needs to be greater than 1\"\n",
    "    \n",
    "    # find similar words to word list\n",
    "    sim_wl = get_sim_words(model, wl, topn=10, debug=debug)\n",
    "    np_sim_wl = np.array(sim_wl)[:,0]\n",
    "    if debug:\n",
    "        print('similar word list', np_sim_wl)\n",
    "\n",
    "    # filter duplicating words\n",
    "    filtered_wl = filter_unique(stemmer, wl, np_sim_wl)\n",
    "    if debug:\n",
    "        print('Filtered word list', filtered_wl)\n",
    "\n",
    "    # rank words\n",
    "    if len(filtered_wl) == 0:\n",
    "        top_w = rank_words(stemmer, wl, word_numtags_map, word_cnt_map, word_cnt_weight=0, word_tag_weight=1, debug=debug)\n",
    "    else:\n",
    "        top_w = rank_words(stemmer, filtered_wl, word_numtags_map, word_cnt_map, word_cnt_weight=0, word_tag_weight=1, debug=debug)\n",
    "    return top_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar word list ['esper' 'shapechanging' 'espers' 'magic-using' 'kekkai' 'paopei' 'Hyōbu'\n",
      " 'magic-user' 'B.A.B.E.L.' 'Tokine']\n",
      "Filtered word list ['Tokine']\n",
      "[0] 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tokine'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl = ['esper', 'espers', 'B.A.B.E.L.', 'Hyōbu' ,'magic-user', 'kekkai',\n",
    " 'shapechanging', 'magic-using', 'paopei', 'meta-human']\n",
    "#wl = ['schadenfreude', 'ephemeral']\n",
    "recommend_words(model, stemmer, wl, word_numtags_map, word_cnt_map, topn=10, word_cnt_weight=0.8, word_tag_weight=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-ea550f4fe5f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrecommend_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_numtags_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_cnt_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-ea550f4fe5f1>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(wl)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrecommend_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_numtags_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_cnt_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-139772ab02cf>\u001b[0m in \u001b[0;36mrecommend_words\u001b[0;34m(model, stemmer, wl, word_numtags_map, word_cnt_map, topn, word_cnt_weight, word_tag_weight, debug)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# find similar words to word list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msim_wl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sim_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mnp_sim_wl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_wl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-076ff24c4f2c>\u001b[0m in \u001b[0;36mget_sim_words\u001b[0;34m(model, wl, topn, debug)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# return top n similar words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwl_w2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/gensim/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilar_by_vector\u001b[0;34m(self, vector, topn, restrict_vocab)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \"\"\"\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     @deprecated(\n",
      "\u001b[0;32m~/anaconda3/envs/gensim/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mlimited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrestrict_vocab\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimited\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtopn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred_words = list(map(lambda wl: recommend_words(model, stemmer, wl, word_numtags_map, word_cnt_map, debug=False), word_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pred(model, word_cnt_map, word_numtag_map, pred_w, true_w):\n",
    "    \"\"\"\n",
    "    Performs 3 metrics evaluations\n",
    "    1. cosine similarity between predicted word versus true word. range -1~1\n",
    "    2. word count difference percentage. 0~1\n",
    "    3. tag count difference percentage. 0~1\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pred_w2vec = model[pred_w]\n",
    "    except:\n",
    "        pred_w2vec = np.zeros((300))\n",
    "    try:\n",
    "        true_w2vec = model[true_w]\n",
    "    except:\n",
    "        true_w2vec = np.zeros((300))\n",
    "    if not np.any(pred_w2vec) or not np.any(true_w2vec):\n",
    "        cos_sim = 0\n",
    "    else:\n",
    "        cos_sim = np.dot(pred_w2vec, true_w2vec)/(np.linalg.norm(pred_w2vec) * np.linalg.norm(true_w2vec))\n",
    "    \n",
    "    try:\n",
    "        true_w_wcnt = word_cnt_map[true_w]\n",
    "        pred_w_wcnt = word_cnt_map.get(pred_w, 0)\n",
    "        print('true_w_wcnt', true_w_wcnt, 'pred_w_wcnt', pred_w_wcnt)\n",
    "        cnt_sim = abs(true_w_wcnt - pred_w_wcnt)/true_w_wcnt\n",
    "    except:\n",
    "        cnt_sim = 0\n",
    "    \n",
    "    try:\n",
    "        true_w_tagcnt = word_cnt_map[true_w]\n",
    "        pred_w_tagcnt = word_cnt_map.get(pred_w, 0)\n",
    "        print('true_w_tagcnt', true_w_tagcnt, 'pred_w_tagcnt', pred_w_tagcnt)\n",
    "        tagcnt_sim = abs(true_w_tagcnt - pred_w_tagcnt)/true_w_tagcnt\n",
    "    except:\n",
    "        tagcnt_sim = 0\n",
    "\n",
    "    print('Cosine similarity', cos_sim, 'cnt_sim', cnt_sim, 'tagcnt_sim', tagcnt_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word List: ['juggernaut', 'jute', 'bungalow', 'pajama', 'loot', 'pundit', 'sherbet', 'avatar', 'guru']\n",
      "Missing word cushi\n",
      "similar word list ['pundit' 'guru' 'bungalow' 'pajama' 'outfit' 'outfits' 'juggernaut'\n",
      " 'avatar' 'superstar' 'gurus']\n",
      "Filtered word list ['outfit', 'outfits', 'superstar']\n",
      "[0, 0, 2] 2\n",
      "predicted word superstar\n",
      "true_w_wcnt 10 pred_w_wcnt 9\n",
      "true_w_tagcnt 10 pred_w_tagcnt 9\n",
      "Cosine similarity 0 cnt_sim 0.1 tagcnt_sim 0.1\n"
     ]
    }
   ],
   "source": [
    "true_w = word_lists[100][-1]\n",
    "wl_len = len(word_lists[100])\n",
    "wl = word_lists[100][0:wl_len-1]\n",
    "print('Word List:', wl)\n",
    "print('Missing word', true_w)\n",
    "pred_w = recommend_words(model, stemmer, wl, word_numtags_map, word_cnt_map, topn=10, word_cnt_weight=0.8, word_tag_weight=0.2)\n",
    "print('predicted word', pred_w)\n",
    "# evaluate prediction\n",
    "eval_pred(model, word_cnt_map, word_numtags_map, pred_w, true_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
