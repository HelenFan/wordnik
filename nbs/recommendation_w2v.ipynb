{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Recommenders Based on Word2Vec and FastText Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import gensim\n",
    "import pickle\n",
    "import json\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./model/wiki.en/wiki.en.vec', binary=False)\n",
    "# model2 = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(path):\n",
    "    df = pd.read_json(path)\n",
    "    for c in df.columns:\n",
    "        df[c] = df[c].apply(lambda x: list(x.values())[0] if type(x) == dict else x)\n",
    "    return df\n",
    "    \n",
    "def get_json(path):\n",
    "    path2 = 'search_counts.json'\n",
    "    with open(path2, 'rb') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def get_pickle(path):\n",
    "    with open(path, 'rb') as handle:\n",
    "        pic = pickle.load(handle, encoding='latin1')\n",
    "    return pic\n",
    "\n",
    "def check_coverage(all_words, mod):\n",
    "    unique_words = set(all_words)\n",
    "    print('Model dictionary size: {}'.format(len(mod.vocab)))\n",
    "    print('Count of unique listed words: {}'.format(len(unique_words)))\n",
    "    print('Intersection of the above two: {}'.format(len([w for w in unique_words if w in mod.vocab])))\n",
    "    print('% of listed words that is in the dictionary: {}'.format(1.0 * len([w for w in unique_words if w in mod.vocab]) / len(unique_words)))\n",
    "    print('Listings of words (non-unique) that are on our model vocabulary: {}'.format(1.0*len([w for w in all_words if w in mod.vocab])/len(all_words)))\n",
    "    \n",
    "def find_words(input_words, mod, n_outputs=1):\n",
    "    return [w[0] for w in mod.most_similar([w for w in input_words if w in mod.vocab],topn=n_outputs) if w[0].isalpha() and w[0] not in input_words][:n_outputs]\n",
    "\n",
    "def find_rank(lst, dic):\n",
    "    lsty = [w for w in lst if w in dic]\n",
    "    lstn = [w for w in lst if w not in dic]\n",
    "    df = pd.DataFrame([dic[w] for w in lsty], index = lsty, columns = ['cnt'])\n",
    "    df = pd.concat([df,pd.DataFrame([0]*len(lstn), index = lstn, columns = ['cnt'])])\n",
    "    df['rank'] = df.cnt.rank(ascending=False)\n",
    "    return df\n",
    "\n",
    "def filter_words(input_, lst):\n",
    "    return [w for w in lst if stemmer.stem(w) not in [stemmer.stem(i) for i in input_]]\n",
    "\n",
    "def find_filter_rank_words(input_, dic1, dic2, dic3, mod, n_outputs_ = 5, multiple = 5):\n",
    "    lst_ = find_words(input_, mod, n_outputs = n_outputs_*multiple)\n",
    "    lst = filter_words(input_, lst_)\n",
    "    rank_l = find_rank(lst, dic = dic1)\n",
    "    rank_s = find_rank(lst, dic = dic2)\n",
    "    rank_t = find_rank(lst, dic = dic3)\n",
    "    df = pd.concat([rank_l, rank_s, rank_t], axis=1).reset_index()\n",
    "    df['rank_combine'] = df['rank'].sum(axis=1)\n",
    "    df['stem'] = df['index'].apply(lambda x: stemmer.stem(x))\n",
    "    df.drop_duplicates(['stem'],inplace=True)\n",
    "    return df.sort_values('rank_combine').iloc[:n_outputs_]['index'].values\n",
    "\n",
    "def predict_(s, mod):\n",
    "    try:\n",
    "        return mod[s]\n",
    "    except:\n",
    "        return np.zeros(300)\n",
    "    \n",
    "def averaging(arrays):\n",
    "    arrays_ = pd.Series(arrays).apply(lambda x: np.reshape(x, (-1, 1)))\n",
    "    return np.mean(np.concatenate(arrays_.values, axis=1), axis=1)\n",
    "\n",
    "def find_lists(test, list_vecs, listed_words, word_lists, mod, n_outputs=3):\n",
    "    test_arrays = pd.Series(test).apply(lambda x: predict_(x, mod))\n",
    "    test_avg = averaging(test_arrays)\n",
    "    dists = {}\n",
    "    for wordlist in list_vecs:\n",
    "        dist = 1 - scipy.spatial.distance.cosine(test_avg, list_vecs[wordlist])\n",
    "        if dist >= 0 and dist <= 1:\n",
    "            dists[dist] = wordlist\n",
    "    ds = sorted(dists.keys(), reverse=True)[:n_outputs]\n",
    "    lists = [dists[d] for d in ds]\n",
    "    for bestlist in lists:\n",
    "        list_name = word_lists[word_lists._id==bestlist].name\n",
    "        creator = word_lists[word_lists._id==bestlist].createdBy\n",
    "        listed_words_ = listed_words[listed_words.wordListId==bestlist].lcword.unique()\n",
    "        print('\\n')\n",
    "        print('List name: {}'.format(list_name.iloc[0]))\n",
    "        print('Creator: {}'.format(creator.iloc[0]))\n",
    "        print('Words in list: {}'.format(listed_words_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickle(path):\n",
    "    with open(path, 'rb') as handle:\n",
    "        pic = pickle.load(handle, encoding='latin1')\n",
    "    return pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listed_words = get_clean_data('new_valid_list_data/valid_listed_words')\n",
    "# word_lists = get_clean_data('new_valid_list_data/valid_list_metadata')\n",
    "# list_vecs = get_pickle('listvecs.pickle')\n",
    "# pop_search = get_json('search_counts.json')\n",
    "# all_words = listed_words.lcword.values\n",
    "pop_listed = dict(listed_words.lcword.value_counts())\n",
    "pop_search = get_json('word_cnts.json')\n",
    "pop_tag = get_json('word_numtags_map.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Checking vocubulary coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dictionary size: 2519370\n",
      "Count of unique listed words: 484355\n",
      "Intersection of the above two: 151439\n",
      "% of listed words that is in the dictionary: 0.31266116794499904\n",
      "Listings of words (non-unique) that are on our model vocabulary: 0.698293419856025\n"
     ]
    }
   ],
   "source": [
    "check_coverage(all_words, mod = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommending New Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['delta','analytics','data','analysis','model','analyze','statistics','database']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['quantitative', 'qualitative', 'visualization', 'methodologies',\n",
       "       'dataset', 'benchmarking', 'predictive', 'analyses',\n",
       "       'datastructure', 'geostatistics'], dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_filter_rank_words(test, pop_listed, pop_search, pop_tag, mod = model, n_outputs_ = 10, multiple = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['quantitative', 'heuristics', 'qualitative', 'metrics',\n",
       "       'visualization', 'methodologies', 'biometrics', 'multivariate',\n",
       "       'forecasting', 'computational'], dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_filter_rank_words(test, pop_listed, pop_search, pop_tag, mod = model, n_outputs_ = 10, multiple = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommending Existing Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "List name: Work Words\n",
      "Creator: mandyshea\n",
      "Words in list: ['analysis' 'analyst' 'analytics' 'analyze' 'verbiage']\n",
      "\n",
      "\n",
      "List name: Words of the future\n",
      "Creator: beohbe\n",
      "Words in list: ['portfolio' 'data' 'research' 'profile' 'grammar challenge']\n",
      "\n",
      "\n",
      "List name: SCIE - mathematics\n",
      "Creator: gulyasrobi\n",
      "Words in list: ['nonparametric' 'nonparametric statistics' 'multivariate analysis'\n",
      " 'partial differential equation' 'multivariate' 'topology' 'stochastic'\n",
      " 'differential equation' 'linear algebra' 'harmonic analysis'\n",
      " 'applied mathematics' 'combinatorial' 'nonlinear' 'computational'\n",
      " 'set theory' 'linear programming' 'parametric' 'numerical analysis'\n",
      " 'group theory' 'statistical method' 'asymptotic' 'mathematical logic'\n",
      " 'discrete' 'probabilistic' 'boolean' 'estimator' 'differential' 'fractal'\n",
      " 'symmetric' 'markov' 'simulation' 'mathematics' 'graphical'\n",
      " 'mathematical' 'extremum' 'algebraic' 'statistical' 'equation'\n",
      " 'algorithm' 'geometric' 'fermat' 'cosine' 'actuary' 'linear' 'randomness'\n",
      " 'analysis' 'algebra' 'numerical' 'statistics' 'geometry' 'dynamics'\n",
      " 'fourier' 'dimensional' 'methodology' 'computation' 'harmonic' 'computer'\n",
      " 'application' 'visualization' 'logic' 'vector' 'categorical' 'regression'\n",
      " 'approximation' 'finite' 'dynamic' 'analytic' 'integral' 'processing'\n",
      " 'optimum' 'theory' 'turbulence' 'spatial' 'math' 'kinetic' 'evolutionary'\n",
      " 'method' 'complexity' 'dimension' 'functional' 'spectral' 'probability'\n",
      " 'compute' 'structured' 'mapping' 'inverse' 'data' 'fracture' 'calculus'\n",
      " 'graphic' 'causal' 'matrix' 'cognitive' 'incidence' 'queue' 'singularity'\n",
      " 'transform' 'aided' 'technique' 'evaluation' 'design' 'norm' 'annals'\n",
      " 'process' 'partial' 'porous' 'linguistic' 'scatter' 'variable' 'sine'\n",
      " 'research' 'philosophy' 'geometrical' 'element' 'computing'\n",
      " 'mathematician' 'chemical' 'calculation' 'phenomenon' 'non' 'monitoring'\n",
      " 'approximate' 'survival' 'fusion' 'triangular' 'unstable' 'random'\n",
      " 'function' 'economics' 'monitor' 'landmark' 'integration' 'bolt' 'hybrid'\n",
      " 'operation' 'stationary' 'structure' 'symbolic' 'latent' 'manifold'\n",
      " 'turbulent' 'physical' 'framework' 'image' 'transport' 'inference'\n",
      " 'artificial' 'practical' 'communications' 'estimation' 'robust'\n",
      " 'essential' 'elements' 'foundation' 'complex' 'automatic' 'trend'\n",
      " 'perspective' 'hypothesis' 'illustrate' 'experiment' 'psychology'\n",
      " 'comprehensive' 'product' 'environmental' 'aid' 'communication'\n",
      " 'intelligence' 'aspect' 'achievement' 'technology' 'formula' 'logical'\n",
      " 'inspection' 'classical' 'transfer' 'devices' 'associate' 'text'\n",
      " 'collect' 'device' 'contemporary' 'strategy' 'fundamental' 'foul'\n",
      " 'parallel' 'principle' 'medium' 'institute' 'survey' 'formal' 'capture'\n",
      " 'invitation' 'network' 'nuclear' 'external' 'measure' 'internal'\n",
      " 'economic' 'practice' 'global' 'intelligent' 'population' 'error'\n",
      " 'permanent' 'proceed' 'income' 'scheme' 'shape' 'solid' 'vision'\n",
      " 'medical' 'machine' 'motion' 'project' 'administration' 'surface'\n",
      " 'non-zero-sumness']\n"
     ]
    }
   ],
   "source": [
    "find_lists(test, list_vecs, listed_words, word_lists, model, n_outputs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
