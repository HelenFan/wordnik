{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Recommenders Based on Word2Vec and FastText Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import gensim\n",
    "import pickle\n",
    "import json\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(path):\n",
    "    \"\"\"Read a json file into pandas dataframe. Clean the df by changing every cell from a dictionary to a number or string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "        Location of the Wordnik json file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas dataframe\n",
    "        Cleaned dataframe of table\n",
    "    \"\"\"\n",
    "    df = pd.read_json(path)\n",
    "    for c in df.columns:\n",
    "        df[c] = df[c].apply(lambda x: list(x.values())[0] if type(x) == dict else x)\n",
    "    return df\n",
    "    \n",
    "def get_json(path):\n",
    "    \"\"\"Read a json file into a dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "        Location of the json file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data : dictionary\n",
    "        Table in a dictionary\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def get_pickle(path):\n",
    "    \"\"\"Get table/dictionary from a pickled file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "        Location of the pickled file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data : dictionary\n",
    "        Table of pickled file in a dictionary\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as handle:\n",
    "        u = pickle._Unpickler(handle)\n",
    "        u.encoding = 'latin1'\n",
    "        d = u.load()\n",
    "    return d\n",
    "\n",
    "def check_coverage(listed_words, mod):\n",
    "    \"\"\"Check the coverage of our word embedding over the listed words. Print results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    listed_words : pandas dataframe\n",
    "        dataframe of the \"listed word\" table from Wordnik\n",
    "        \n",
    "    mod : fasttext word embeddings\n",
    "        model, or the word embeddings from Fasttext\n",
    "    \"\"\"\n",
    "    all_words = listed_words.lcword.values\n",
    "    unique_words = set(all_words)\n",
    "    print('Model dictionary size: {}'.format(len(mod.vocab)))\n",
    "    print('Count of unique listed words: {}'.format(len(unique_words)))\n",
    "    print('Intersection of the above two: {}'.format(len([w for w in unique_words if w in mod.vocab])))\n",
    "    print('% of listed words that is in the dictionary: {}'.format(1.0 * len([w for w in unique_words if w in mod.vocab]) / len(unique_words)))\n",
    "    print('Listings of words (non-unique) that are on our model vocabulary: {}'.format(1.0*len([w for w in all_words if w in mod.vocab])/len(all_words)))\n",
    "    \n",
    "def find_words(input_words, mod, n_outputs=1):\n",
    "    \"\"\"Given an input list of words, find recommended words based on similarities.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_words : list\n",
    "        list of words that the recommendation will be based on\n",
    "        \n",
    "    mod : fasttest word embedding\n",
    "    \n",
    "    n_outputs: integer\n",
    "        the number of recommended (most similar) words wanted\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        list of words recommended\n",
    "    \n",
    "    \"\"\"\n",
    "    out = [w[0] for w in mod.most_similar([w for w in input_words if w in mod.vocab],topn=n_outputs) if w[0].isalpha() and w[0] not in input_words][:n_outputs]\n",
    "    return out\n",
    "\n",
    "def find_rank(lst, dic):\n",
    "    \"\"\"Given a list of (recommended) words, rank them based on popularity.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lst : list\n",
    "        list of words\n",
    "    dic : dictionary\n",
    "        this is the \"popularity dictionary\", which shows the number of times each word is searched/tagged/listed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas dataframe\n",
    "        table containing the original list of words, and their popularity ranking\n",
    "    \"\"\"\n",
    "    lsty = [w for w in lst if w in dic]\n",
    "    lstn = [w for w in lst if w not in dic]\n",
    "    df = pd.DataFrame([dic[w] for w in lsty], index = lsty, columns = ['cnt'])\n",
    "    df = pd.concat([df,pd.DataFrame([0]*len(lstn), index = lstn, columns = ['cnt'])])\n",
    "    df['rank'] = df.cnt.rank(ascending=False)\n",
    "    return df\n",
    "\n",
    "def filter_words(input_, lst):\n",
    "    \"\"\"Filter out words that have the same stem\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_ : list\n",
    "        list of input words\n",
    "        \n",
    "    lst : list\n",
    "        list of recommended words\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        list of trimmed down version of lst that does not contain words that share same stems with input_\n",
    "    \"\"\"\n",
    "    out = [w for w in lst if stemmer.stem(w) not in [stemmer.stem(i) for i in input_]]\n",
    "    return out\n",
    "\n",
    "def find_filter_rank_words(input_, dic1, dic2, dic3, mod, n_outputs_ = 5, multiple = 5):\n",
    "    \"\"\"Given list of input words, find a list of recommended words based on similarity, then rank tham based on 3 Popularity measures.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_ : list\n",
    "        input words from user\n",
    "        \n",
    "    dic1, dic2, dic3 : dictionary\n",
    "        dictionaries of word popularity\n",
    "        \n",
    "    mod : fasttext word embeddings\n",
    "    \n",
    "    n_output_ : integer\n",
    "        the number of words in the final recommendation, after filtering and ranking\n",
    "        \n",
    "    multiple : integer\n",
    "        scalar multiplied to n_output_ to get the number of most similar words before the filtering by popularity\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    rec : list\n",
    "        list of final recommendation of words\n",
    "    \"\"\"\n",
    "    lst_ = find_words(input_, mod, n_outputs = n_outputs_*multiple)\n",
    "    lst = filter_words(input_, lst_)\n",
    "    rank_l = find_rank(lst, dic = dic1)\n",
    "    rank_s = find_rank(lst, dic = dic2)\n",
    "    rank_t = find_rank(lst, dic = dic3)\n",
    "    df = pd.concat([rank_l, rank_s, rank_t], axis=1).reset_index()\n",
    "    df['rank_combine'] = df['rank'].sum(axis=1)\n",
    "    df['stem'] = df['index'].apply(lambda x: stemmer.stem(x))\n",
    "    df.drop_duplicates(['stem'],inplace=True)\n",
    "    rec = df.sort_values('rank_combine').iloc[:n_outputs_]['index'].values\n",
    "    return rec\n",
    "\n",
    "def predict_(s, mod):\n",
    "    \"\"\"Given a word, find its embedding. This function is used in List Recommendation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return mod[s]\n",
    "    except:\n",
    "        return np.zeros(300)\n",
    "    \n",
    "def averaging(arrays):\n",
    "    \"\"\"Given an array of word embeddings vectors, find the average of all vectors. This function is used in List Recommendation.\n",
    "    \"\"\"\n",
    "    arrays_ = pd.Series(arrays).apply(lambda x: np.reshape(x, (-1, 1)))\n",
    "    return np.mean(np.concatenate(arrays_.values, axis=1), axis=1)\n",
    "\n",
    "def find_lists(test, list_vecs, listed_words, word_lists, mod, n_outputs=3):\n",
    "    \"\"\"Given a list of words, recommend a list from the existing Wordnik lists\n",
    "    \"\"\"\n",
    "    test_arrays = pd.Series(test).apply(lambda x: predict_(x, mod))\n",
    "    test_avg = averaging(test_arrays)\n",
    "    dists = {}\n",
    "    for wordlist in list_vecs:\n",
    "        dist = 1 - scipy.spatial.distance.cosine(test_avg, list_vecs[wordlist])\n",
    "        if dist >= 0 and dist <= 1:\n",
    "            dists[dist] = wordlist\n",
    "    ds = sorted(dists.keys(), reverse=True)[:n_outputs]\n",
    "    lists = [dists[d] for d in ds]\n",
    "    for bestlist in lists:\n",
    "        list_name = word_lists[word_lists._id==bestlist].name\n",
    "        creator = word_lists[word_lists._id==bestlist].createdBy\n",
    "        listed_words_ = listed_words[listed_words.wordListId==bestlist].lcword.unique()\n",
    "        print('\\n')\n",
    "        print('List name: {}'.format(list_name.iloc[0]))\n",
    "        print('Creator: {}'.format(creator.iloc[0]))\n",
    "        print('Words in list: {}'.format(listed_words_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and model (word embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./model/wiki.en/wiki.en.vec', binary=False)\n",
    "# upload word embedding from fasttext \"wiki.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_words = get_clean_data('new_valid_list_data/valid_listed_words')\n",
    "# pandas dataframe of Wordnik table of listed words\n",
    "word_lists = get_clean_data('new_valid_list_data/valid_list_metadata')\n",
    "# pandas dataframe of Wordnik table of word lists\n",
    "list_vecs = get_pickle('listvecs.pickle')\n",
    "# dictionary of vector representation of all the existing word lists (average of the embeddings of all the words in the list)\n",
    "# key : id of the list; value : vector of embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_listed = dict(listed_words.lcword.value_counts())\n",
    "pop_search = get_json('word_cnts.json')\n",
    "pop_tag = get_json('word_numtags_map.json')\n",
    "# these 3 dictionaries show the number of times every word is searched/tagged/listed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Checking vocubulary coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dictionary size: 2519370\n",
      "Count of unique listed words: 484355\n",
      "Intersection of the above two: 151439\n",
      "% of listed words that is in the dictionary: 0.31266116794499904\n",
      "Listings of words (non-unique) that are on our model vocabulary: 0.698293419856025\n"
     ]
    }
   ],
   "source": [
    "check_coverage(listed_words, mod = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommending New Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['delta','analytics','data','analysis','model','analyze','statistics','database']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-86103a8a3ed0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfind_filter_rank_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpop_listed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpop_search\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpop_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_outputs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-a6a240530aa0>\u001b[0m in \u001b[0;36mfind_filter_rank_words\u001b[0;34m(input_, dic1, dic2, dic3, mod, n_outputs_, multiple)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m    161\u001b[0m     \u001b[0mlst_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_outputs_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlst_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0mrank_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdic1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mrank_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdic2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a6a240530aa0>\u001b[0m in \u001b[0;36mfilter_words\u001b[0;34m(input_, lst)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \"\"\"\n\u001b[1;32m    134\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_filter_rank_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_outputs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pit' is not defined"
     ]
    }
   ],
   "source": [
    "find_filter_rank_words(test, pop_listed, pop_search, pop_tag, mod = model, n_outputs_ = 10, multiple = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_filter_rank_words(test, pop_listed, pop_search, pop_tag, mod = model, n_outputs_ = 10, multiple = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommending Existing Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "find_lists(test, list_vecs, listed_words, word_lists, model, n_outputs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
