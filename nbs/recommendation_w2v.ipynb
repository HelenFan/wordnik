{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Python libraries and project datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import gensim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'new_valid_list_data/valid_listed_words'\n",
    "df = pd.read_json(path)\n",
    "for c in df.columns:\n",
    "    df[c] = df[c].apply(lambda x: x.values()[0] if type(x) == dict else x)\n",
    "\n",
    "path1 = 'new_valid_list_data/valid_list_metadata'\n",
    "df1 = pd.read_json(path1)\n",
    "for c in df1.columns:\n",
    "    df1[c] = df1[c].apply(lambda x: x.values()[0] if type(x) == dict else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = FastText embeddings trained on English Wikipedia documents\n",
    "\n",
    "model2 = Word2Vec embeddings trained on Google News documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./model/wiki.en/wiki.en.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA: Checking vocubulary coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = df.lcword.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(all_words=all_words, mod = model):\n",
    "    print('Model dictionary size: {}'.format(len(mod.vocab)))\n",
    "    print('Count of unique listed words: {}'.format(len(all_words)))\n",
    "    print('Intersection of the above two: {}'.format(len([w for w in all_words if w in mod.vocab])))\n",
    "    print('% of listed words that is in the dictionary: {}'.format(1.0 * len([w for w in all_words if w in mod.vocab]) / len(all_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage of FastText pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dictionary size: 2519370\n",
      "Count of unique listed words: 484355\n",
      "Intersection of the above two: 151439\n",
      "% of listed words that is in the dictionary: 0.312661167945\n"
     ]
    }
   ],
   "source": [
    "check_coverage(mod = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage of Word2Vec pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dictionary size: 3000000\n",
      "Count of unique listed words: 484355\n",
      "Intersection of the above two: 76517\n",
      "% of listed words that is in the dictionary: 0.157977103571\n"
     ]
    }
   ],
   "source": [
    "check_coverage(mod = model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:  List of words with length >= 1\n",
    "\n",
    "Output: List of recommended words with length = \"n_outputs\"; \n",
    "\n",
    "Recommended words are selected from the whole pretrained dictionary based on Similarity to Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words(input_words = ['testing'], n_outputs = 1, mod = model):\n",
    "    return [w[0] for w in mod.most_similar([w for w in input_words if w in mod.vocab],topn=n_outputs+10) if w[0].isalpha() and w[0] not in list(input_words)][:n_outputs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['data','computer','engineering','software','machine','computing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing of Recommendation 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'computery']"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_words(test, n_outputs=1, mod = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:  List of words with length >= 1\n",
    "\n",
    "Output: List of recommended wordlists with length = n_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('listvecs.pickle', 'rb') as handle:\n",
    "    dic = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_(s, mod=model):\n",
    "    try:\n",
    "        return mod[s]\n",
    "    except:\n",
    "        return np.zeros(300)\n",
    "def averaging(arrays):\n",
    "    arrays_ = pd.Series(arrays).apply(lambda x: np.reshape(x, (-1, 1)))\n",
    "    return np.mean(np.concatenate(arrays_.values, axis=1), axis=1)\n",
    "def find_lists(test, n_outputs=3, dic=dic):\n",
    "#     test = s.translate(None, string.punctuation).split()\n",
    "    test_arrays = pd.Series(test).apply(lambda x: predict_(x))\n",
    "    test_avg = averaging(test_arrays)\n",
    "    dists = {}\n",
    "    for wordlist in dic:\n",
    "        dist = 1 - scipy.spatial.distance.cosine(test_avg, dic[wordlist])\n",
    "        if dist >= 0 and dist <= 1:\n",
    "            dists[dist] = wordlist\n",
    "    ds = sorted(dists.keys(), reverse=True)[:n_outputs]\n",
    "#     print(ds)\n",
    "    lists = [dists[d] for d in ds]\n",
    "    for bestlist in lists:\n",
    "        list_name = df1_[df1_._id==bestlist].name\n",
    "        creator = df1_[df1_._id==bestlist].createdBy\n",
    "        listed_words = df[df.wordListId==bestlist].lcword.unique()\n",
    "        print('\\n')\n",
    "        print('List name: {}'.format(list_name.iloc[0]))\n",
    "        print('Creator: {}'.format(creator.iloc[0]))\n",
    "        print('Words in list: {}'.format(listed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing of Recommendation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "List name: Gleichen\n",
      "Creator: heidikraut\n",
      "Words in list: [u'information' u'computer' u'manipulation' u'hand']\n",
      "\n",
      "\n",
      "List name: computer words\n",
      "Creator: edwardvielmetti\n",
      "Words in list: [u'computer' u'floppy disk' u'keyboard' u'feed' u'memex' u'recursive'\n",
      " u'kernel' u'debug' u'dogfooding' u'bootstrapping' u'ansible']\n",
      "\n",
      "\n",
      "List name: Technofancy\n",
      "Creator: pliu32\n",
      "Words in list: [u'chipset' u'kernel' u'motherboard' u'network' u'bios' u'microprocessor'\n",
      " u'ram' u'rom' u'register' u'pipelining' u'smartphone' u'programmable'\n",
      " u'laptop' u'wireless' u'anti-aliasing' u'anisotropic' u'latency' u'lag'\n",
      " u'ping' u'deinterlace' u'spatiotemporal' u'biometric' u'encryption'\n",
      " u'authentication' u'cipher' u'hypervisor' u'stack' u'heap' u'debug'\n",
      " u'pointer' u'dereference' u'operand' u'packet' u'firewall'\n",
      " u'multiplexing' u'symlink']\n",
      "\n",
      "\n",
      "List name: tech words\n",
      "Creator: paulosuzart\n",
      "Words in list: [u'soa' u'environment' u'production' u'architecture' u'architect'\n",
      " u'language' u'java' u'application' u'integration' u'deploy' u'deployment'\n",
      " u'install' u'cluster' u'failover' u'performance' u'transaction'\n",
      " u'exception' u'timeout' u'error' u'failure' u'persistence' u'validation'\n",
      " u'service' u'crud' u'component' u'model' u'object' u'code' u'process'\n",
      " u'business' u'resource' u'cache' u'distributed' u'data' u'query' u'table'\n",
      " u'base' u'developtment' u'runtime' u'run']\n",
      "\n",
      "\n",
      "List name: andreia's Words\n",
      "Creator: andreia\n",
      "Words in list: [u'expertise' u'chocolate' u'aggregation' u'folksonomy' u'tagging'\n",
      " u'scalability' u'web' u'productivity' u'planning' u'programming' u'hack'\n",
      " u'love' u'collaborating' u'essential' u'idea' u'research' u'blogging'\n",
      " u'blogosphere' u'network' u'integration' u'theory' u'proactive' u'navy'\n",
      " u'pervasive' u'relativity' u'simplicity' u'usability']\n"
     ]
    }
   ],
   "source": [
    "find_lists(test, n_outputs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
